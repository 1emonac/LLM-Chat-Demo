{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/1emonac/LLM-Chat-Demo/blob/main/1.%EC%9E%90%EC%97%B0%EC%96%B4_%EC%B2%98%EB%A6%AC_%EC%98%A4%EB%B2%84%EB%B7%B0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 자연어 처리란?\n",
        "- 정의\n",
        "  1. 자연어 처리는 인간의 언어를 컴퓨터가 이해하고 처리할 수 있도록 하는 분야\n",
        "  2. 기계번역, 텍스트 분류, 질의 응답 시스템 등 다양한 응용 프로그램을 개발\n",
        "\n",
        "- 주요 Task\n",
        "  1. Text Classification: 주어진 텍스트를 **카테고리(클래스)로 분류**하는 작업 (스팸 메일인지 아닌지)\n",
        "  2. Sentiment Analysis: 주어진 문장의 **감정(감성)을 분류**하는 작업 (긍정적인 문장인지 부정적인 문장인지)\n",
        "  3. Question Answering: 사용자가 입력한 질문에 대해 **적절한 응답을 생성**하는 작업(extracctive or abstractive)\n",
        "  4. Maxhine Translation: 두 개의 다른 언어 간의 번역을 수행하는 작업(영어로 된 문서를 한국어로 번역)\n",
        "  5. Name Entity Recognition: 주어진 텍스트에서 특정 단어나 구문의 **엔티티**를 찾아내는 작업(이메일 주소나 전화번호를 찾기)\n",
        "  6. Summarization: 주어진 문서를 간단하게 **요약**하는 작업(긴 문서를 짧게 요악하거나 핵심 내용을 강조"
      ],
      "metadata": {
        "id": "3ZISlEIbg7GO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tokenizer 1.0 - KoNLPY**\n",
        "\n",
        "- 토큰(Token)\n",
        "  문장이나 텍스트에서 의미를 가진 가장 작은 단위\n",
        "\n",
        "- 토크나이저\n",
        "  1. KoNLPY\n",
        "  - 한국어 정보처리를 위한 파이썬 패키지\n",
        "    1. Hannanum\n",
        "    2. KKma\n",
        "    3. Komoran\n",
        "    4. Mecab (Pecab도 많이 씀) - 속도가 빠름\n",
        "    5. Okt (Open Korean Text (트위터))\n",
        "\n",
        "    - 한글 **형태소 분석**을 위해 주로 사용\n",
        "    - BERT 이전에는 많이 사용 했었음\n",
        "      - 요즘은 **Morpheme base BBPE** 쓸 때 혹은 **검색엔진** 색인 할 때 많이 사용\n",
        "\n",
        "  2. Wordpiece\n",
        "  3. BPE"
      ],
      "metadata": {
        "id": "2at6VJiym3F8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. 자연어처리 모델 발전과정** (From Word2Vec to GPT-3)\n",
        "\n",
        "- 언어 모델이 뜨기 전 자연어처리?\n",
        "  \n",
        "  **패러다임1: Word2Vec, GloVe, FastText (word embeddings)**\n",
        "  - CBOW(Continuous Bag-of-Words), Skip-Gram? 구글에서 제안한 방법론\n",
        "    - Efficient Estimation of Word Representations in Vector Space (논문)\n",
        "    - Distributed Representations of Words and Phrasese and their Compositionality (논문)\n",
        "    \n",
        "      - 예) Word Emvedding Analogies: Understanding\n",
        "            King - Man + Woman = Queen\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "  - Glove(Global Vectors for Word Representation) (논문)\n",
        "    - Stanford에서 개발한 방법론\n",
        "    - Word2Vec을 보완한 방법론 (실제로는 여러 방법론을 다 써봐야 함)\n",
        "    - 각 단어의 빈도수를 담은 행렬을 통해 통계정보를 기반으로 차원축소 (Truncated SVD)를 통해 잠재된 의미를 끌어내는 방법론\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "  - FastText\n",
        "    - Enriching Word Vectors with Subwod Information (논문)\n",
        "    - 메타(구 페이스북)에서 개발한 방법론\n",
        "    - Word2Vec과 달리 하나의 단어 안에도 여러 단어가 존재한다고 보는 n-gram 기반 서브워드 방법론\n",
        "      - subword를 쓰므로 OOV(Out Of Vocabulary) 문제를 조금 더 잘 해결할 수 있음\n",
        "      - 한국어의 경우 음절 단위 또는 자모단위로도 임베딩하는 시도가 있었음\n",
        "    - Gensim library로 많이 사용되었음\n",
        "    - 최근에는 LLM 데이터를 간단하게 필터링하는 쪽에서도 많이 사용됨\n",
        "\n",
        "      - 예1) # FastText 음절(글자) 단위\n",
        "        \"안녕하세요\" > \"<\"안녕\", \"안녕하\", \"녕하세\" ... \"하세요\", \"세요\">\"\n",
        "      - 예2) # FastText 자모(초성, 중성, 종성) 단위\n",
        "        \"안녕하세요\" > \"<\"ㅇㅏ\", \"ㅇㅏㄴ\", ... \"ㅇㅛ\">\"\n",
        "\n",
        "** 임베딩(Embedding) **\n",
        "- \"임베딩\"은 **콘텐츠를 숫자화**해서 **의미공간으로 맵핑**하는 것. 내용의 길이에 관계없이 같은 크기의 \"부동 소수점 숫자의 배열\"로 변환하는 것. 임베딩 된 공간에서의 위치는 콘텐츠의 \"의미론적 의미(Semantic Meaning)\"를 나타냄. 개별 숫자가 무엇을 의미하는지 완전히 이해 할 순 없지만, 콘텐츠에 대한 유용한 정보를 제공. **좋은 임베딩, 좋은 Representation**을 만들어 낼수록 **좋은 모델 및 결과**가 나오게 됨"
      ],
      "metadata": {
        "id": "XHJcAADLsdAu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**패러다임2: RNN, CNN, Seq2Seq, Attention**\n",
        "  - RNN (Recurrent Neural Network)\n",
        "    - 전통적인 언어모델을 대체하는 시도\n",
        "      - n-gram기반 LM은 n이 커질수록 메모리와 계산이 많이 필요하고 긴 범위의 의존성을 모델링하지 못함\n",
        "    - 학습 시에는 출력 값을 입력으로 넣지 않고 정답을 넣어주는 teacher frocing(교사 강요) 사용\n",
        "    - Vanishing/exploding gradient 이슈 및 장기의존성(Long-term dependency) 문제 등이 있었음\n",
        "      - GRU(Gated Recurrent Units), LSTM(Long-short-term-memories), Bi-GRU, Bi-LSTM 등 다양한 형태의 개선된 버전의 모델이 등장\n",
        "\n",
        "    - TextCNN(Text Convolution Neural Network)\n",
        "      - 컴퓨터 비전에서 사용되던 CNN 아키텍쳐를 자언어처리에 적용\n",
        "      - 다양한 분류 도메인에서 좋은 성능을 보였음\n",
        "    - Convolutuinal Neural Networks for Sentence Classificaton (논문)\n",
        "\n",
        "   [RNN의 다른 변형]\n",
        "\n",
        "    - RNN\n",
        "      - LSTM\n",
        "      - GRU\n",
        "      - ELMo (Pretrained LM의 장점 언급)\n",
        "    - CNN\n",
        "      - TextCNN\n",
        "    - Seq2Seq\n",
        "      - RNN style\n",
        "      - CNN style\n",
        "    - Attention\n",
        "      - context vector style\n",
        "\n",
        "    - (Attention) Neural MAchine Translation by Jointly Learning to Allgn.. (논문)\n",
        "    - (분류 태스크) Hierarchical Attention Networks for Document Classification (논문)\n",
        "\n",
        "    **Transformer**\n",
        "    \"Natural Language Processing can be divided into 'before Transformer' and 'after Transformer'\"\n",
        "    - 자연어 처리는 트랜스포머의 전후로 나뉜다고 해도 과언이 아님!\n"
      ],
      "metadata": {
        "id": "2_iPLIMfzdP2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "http://nlp.seas.harvard.edu/2018/04/03/attention.html\n",
        "\n",
        "**패러다임3: Transformer**\n",
        "- > Attention만 있으면 된당!\n",
        "- Attention mechanism을 사용하는 Encoder-Decoder 구조(Seq2Seq 구조)의 아키텍쳐\n",
        "  - 장점\n",
        "    - 입력 token sequence 내에서 거리 상관없이 dependency modeling 가능\n",
        "      - 토큰 간 global dependency 고려 가능\n",
        "    - 기존 RNN, CNN 모델과 비교 시 병렬처리, 장거리 토큰 고려 등 장점이 많음\n",
        "    - inductive bias가 낮음\n",
        "\n",
        "    - Transformer Block 순서\n",
        "      - 입력 > MHA(Self-Attention) > Add & Norm > Feed-Forward > Add & Norm\n",
        "\n",
        "    - Self Transform\n",
        "      - Query, Key, Value\n",
        "        - Key와 Value는 같은 토큰을 가리킴\n",
        "        - Query는 다른 토큰을 가리킴\n",
        "        - Key와 Query 토큰에서 계산된 Attention 값을 Value Tensor에 곱해서 보정해줌\n",
        "\n",
        "    - Transforme는 RNN, CNN과 비교할 때 계산량은 많지만 병렬화와 멀리 있는 토큰을 고려하기에 유리함"
      ],
      "metadata": {
        "id": "lWBfCrbf8mjm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tokenizer 2.0 - wordpiece**\n",
        "\n",
        "- Word Piece Model(WPM) a.k.a wordpiece\n",
        "  - Subword Tokenizer의 종류 중 하나\n",
        "  - Google에서 만든 BERT(2018) 학습을 위해 사용됨\n",
        "  - BPE와 비슷한 방법으로 학습됨\n",
        "    - small vocab (base vocab) 으로부터 시작\n",
        "    - prefix로 ## 사용\n",
        "    - 학습 순서\n",
        "      [1] 각 단어를 prefix를 더해서 모두 쪼갬 (Bottom up 방식)\n",
        "          - 예) \"word\" -> w## o##r ##d\n",
        "      [2] BPE와 같이 merge rule을 학습\n",
        "          (차이점은 어떤 pair를 합칠지 결정하는 것이 다름)\n",
        "\n",
        "        - 가장 빈도수가 높은 pair가 아닌 각 pair당 아래의 수식으로 점수를 계산\n",
        "          - score = (freq_of_pair) / (freq_of_first_element x freq_of_second_element)\n",
        "          - pair의 빈도를 각 요소의 빈도에 대한 곱으로 나누어 계싼\n",
        "        - 점수가 높으면 pair가 자주 나타나면서 각 구성 글자는 전체적으로 적게 나타나니 pair는 병합\n",
        "        - 점수가 낮으면 pair가 덜 나타나거나 각 구성 글자가 (서브워드로) 엄청 많이 나타나니 병합하지 않고 그대로 줌\n",
        "        - 점수가 높다 == pari가 잘 나온느 편이니 합치자\n",
        "        - 아예 딱 1번 등장한 캐릭터의 구성은 합쳐짐! 점수가 1로 제일 높음 = 1/(1*1*..*1)\n",
        "\n",
        "  - BPE믄 빈도수 기준으고 wordpiece는 우도(likehood)의 개념\n",
        "  - https://ratsgo.github.io/nlpbook/docs/preprocess/bpe/\n"
      ],
      "metadata": {
        "id": "-9AYEMeeFaHU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Byte-Pair Encoding(BPE)**\n",
        "  - 정보를 압축하는 알고리즘으로 제안(1994) 되었으나 대표적으로 번역 태스크와 GPT(2018)모델에서 tokenization 깁버으로 활용됨\n",
        "  - base vocab을 구성한 뒤 빈도수 기준 병합 우선순위가 높은 pair를 vocab애 추가하는 방식으로 구성\n",
        "  - (Karpathy의 minbpe도 참고하기에 좋음)\n",
        "     - https://github.com/karpathy/minbpe)\n",
        "\n",
        "  - 학습 순서\n",
        "    - 문서로부터 unique 단어 집합을 구함\n",
        "    - 각 단어는 각 단어를 구성하는 단위(ex. ASCII or Unicode character)로 쪼개짐 (Bottom-Up 방식)\n",
        "    - GPT-2, RoBERTa쪽에서는 단어를 Unicode characters로 보지 않고 byte 단위로 봐서 unk를 회피함 (Byte-level BPE(BPE))\n",
        "      - byte는 bit 시퀀스이므로 2^8 = 256 (1바이트 = 8비트) 사이즈를 base vocab의 크기로. 둬서 바이트단위의 문자를 표현할 수 있음\n",
        "    - base vocab을 구성한 뒤 추가되는 토큰들을 설정한 vocab size까지 merge 하는 과정을 거침\n",
        "      - merge는 단어를 구성하는 most frequent pair를 찾은 후 하나의 새로운 토큰으로 치환하는. 과정임\n",
        "      - 등장하는 순서가 같은 경우 알파벳 순으로 병합됨\n",
        "  \n",
        "  **유니코드**\n",
        "    - 국제표준문자표로 현존하는 문자 인코딩 방법들을 모두 유니'코드'로 표현하는 것이 목표\n",
        "    - 현재는 149,813 개의 코드포인트 지원\n",
        "      - (코드 포인트는 텍스트를 표현하기 위한 시스템(예, 유니코드)에서 추상 문자를 나타내기 위해 할당된 숫자. 유니코드에서, 코드 포인트는 \"U+1234\" 형식으로 표현됨. 여기서 \"1234\"는 할당된 숫자임. 예를 들어, 문자 \"A\"dpsms U+0041이라는 코드 포인트가 할당)\n",
        "\n",
        "    - UTF-8 인코딩\n",
        "      - 유니코드 한 문자를 나타내기 위해 1바이트에서 4바이트까지를 사용한다. 예를 들어, U+0000부터 U+007F 범위에 있는 ASCII 문자들은 UTF-8에서 1바이트만으로 표시됨.\n",
        "\n",
        "    - 유니코드(문자)를 UTF-8 방식으로 인코딩하면 bytes 타입이 됨(1~4바이트 내로 표현)\n",
        "    - b'는 bytes 리터럴이라고 하는데 이렇게 b'가 있으면 bytes 객체라고 생각하면 됨\n",
        "    - 뒤에 쓰여있는 \\x는 이스케이프 문자로 뒤에 오는 수가 16진수 라는 것을 의미\n",
        "\n",
        "    - UTF-8 (유니코드의 인코딩 방식)을 native로 사용하게 되면\n",
        "      - string -> byte stream이 됨\n",
        "      - 즉, byte 단위로 string을 표현할 수 있게 됨\n",
        "      - 1 byte는 8 but dla\n",
        "      - 8 bit는 2*2*2*2*2*2*2*2 = 256\n",
        "      - byte-level로 보면 vocab을 256개만 쓰면 byte stream으오 strinbg sequence 표현 가능!\n",
        "      - 하지만 transformer에서 들어가는 입력이 너무 길어짐(attention overhead 켜짐)\n",
        "      - BPE(Byte Pair Encoding)로 압축 필요"
      ],
      "metadata": {
        "id": "_CsQFjOyP1Hg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"abcde 안녕하세요 반갑습니다\"\n",
        "tokens = text.encode('utf-8') # raw bytes\n",
        "tokens_codepoint = list(tokens)\n",
        "print(f\"text: {text}\")\n",
        "print(f\"length of text: {len(text)}\")\n",
        "print(f\"tokens: {tokens}\")\n",
        "print(f\"tokens_code point: {tokens_codepoint}\")\n",
        "print(f\"length of code point: {len(tokens_codepoint)}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5YuCwoIrSpHt",
        "outputId": "9201a8df-f0f3-43a6-aec6-4285e36021bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "text: abcde 안녕하세요 반갑습니다\n",
            "length of text: 17\n",
            "tokens: b'abcde \\xec\\x95\\x88\\xeb\\x85\\x95\\xed\\x95\\x98\\xec\\x84\\xb8\\xec\\x9a\\x94 \\xeb\\xb0\\x98\\xea\\xb0\\x91\\xec\\x8a\\xb5\\xeb\\x8b\\x88\\xeb\\x8b\\xa4'\n",
            "tokens_code point: [97, 98, 99, 100, 101, 32, 236, 149, 136, 235, 133, 149, 237, 149, 152, 236, 132, 184, 236, 154, 148, 32, 235, 176, 152, 234, 176, 145, 236, 138, 181, 235, 139, 136, 235, 139, 164]\n",
            "length of code point: 37\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Tiktoken\n",
        "  - OpenAI에서 개발한 BPE Tokenizer\n",
        "  - ChatGPT 등 OpenAI 모델에서 사용됨\n",
        "  - tiktokenizer라는 titoken의 playground도 존재\n",
        "  - 한글보다는 영어 단어가 더 잘 되는 편\n",
        "    - https://tiktokenizer.vercel.app/\n",
        "  \n",
        "  - SentencePiece\n",
        "    - BPE의 개념을 구현한 토크나이저 라이브러리\n",
        "    - BERT나 T5, LLaMA 등을 학습할 때 사용한 토크나이저\n",
        "    - Whitespace에 대한 Meta Symbol로 \"__\" (U+2581)를 사용함\n",
        "    - 다양한 옵션 제공\n",
        "      - https://github.com/google/sentencepiece/blob/master/doc/options.md\n",
        "      - BBPE를 지원하진 않지만, Unknow token을 UTF-8로 인코딩해서(1~4바이트로 표현가능) OOV를 막는 옵션이 있음(byte_fallback=True)"
      ],
      "metadata": {
        "id": "9zxirZ19UhU1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**토크나이저 구성**\n",
        "\n",
        "- Normalization > Pre-tokenization > Mode > Post-processing\n",
        "\n",
        "- Nomalization\n",
        "  - 디코더의 경우 생성을 위해 입력의 형태를 보존하는 것이 중요하기 때문에 원본이 유지되는 NFC가 좋음\n",
        "- Pretokenization\n",
        "  - UnicodeScripts: This pre-tokenizer splits on characters that belong to diffenrent language family It roughly follows\n",
        "    - https://github.com/google/sentencepiece/blob/master/data/Scipts.txt\n",
        "  - Punctuatuon(behavior='isolated') : 아래 구두점에 대해서 각각 분리\n",
        "    - '!\"#$%^₩'()*+,-./:;<=>?@[₩₩]^_{|}~\n",
        "  - chatGPT regex\n",
        "\n",
        "  - Digits(individual_difits=True): 모든 숫자에 대해서 각각 분리(수학 관련 Task에 유리할 것으로 판단)\n",
        "  - ByteLevel(add_prefix_space=False, use_regex=True): 바이트레벨로 변환\n",
        "    - add_prefix_space: True로 하면 decode 시 기존 텍스트 복원 안되므로 False 적용\n",
        "    - use_regex: GPT2에서 사용한 whitespace에 대한 split regex 적용\n"
      ],
      "metadata": {
        "id": "MUYzMwxzWThT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Language Model**\n",
        "- 언어모델이란\n",
        "  - 주어진 문장을 구성하는 단어 또는 토큰에 대한 일련의 확률분포를 모델링하는 기법\n",
        "  - **주어진 문장의 다음 단어를 예측하거나, 문장 내 마스킹한 단어를 예측하는 것을 목표**로 학습\n",
        "  - 사전 학습된 언어모델은 굉장히 좋은 Representation(Embedding) 보유하게 됨\n",
        "\n",
        "  - R-Denoising\n",
        "    - 문장 중간의 빈칸 단어를 맞추기\n",
        "  - S-Denoising\n",
        "    - 꽤 긴 문장응 맞추는 것\n",
        "  - X-Denoising\n",
        "   - R + S-Denoising, 빈칸의 단어와 문장을 맞추기"
      ],
      "metadata": {
        "id": "d51ov4L_b8i4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- BERT: 2018년 구글에서 발표한 (Transformer Encoder Block을 사용한 NLU모델)\n",
        "  - Left-to-Right (Autoregressive)한 방법이 아닌 Bidirectional한 방법으로 모델 학습 시도\n",
        "    - MLM (Masked Language Model) (15%)\n",
        "  - Token level뿐만 아니라 문장 간의 관계도 학습하는 Sentence level 학습 시도\n",
        "    - NSP (Next Sentence Prediction)\n",
        "  - BERT: Pre-training of Deep Bidirectional Transformers for ... (논문)\n",
        "\n",
        "- Language Models are Unsupervised Multitask Learners (논문)\n",
        "- GPT: OpenAI에서 발표한 모델 (Transformer Decoder Block을 사용한 AR 모델)\n",
        "  - Left-to-Right (Autoregressive)힌 방법으로 모델 학습 시도\n",
        "    - Next token을  prediction 해서 모델을 학습 시킴 (다음에 나올 토큰이 뭐냐를 맞춰서 학습시킴)\n",
        "    \n",
        "- GPT2: 2018년 OpenAI에서 발표한 모델(Transformer Decoder Block을 사용한 AR모델)\n",
        "  - 데이터(40GB)와 모델 사이즈(1.5B까지)를 최대 10배 이상 키운 모델\n",
        "  - 작은 모델에서는 볼 수 없었던 Multitask에 대한 zero-shot능력이 있음을 발견\n",
        "\n",
        "- GPT3: 2020년 OpenAI에서 발표한 모델(Transformer Decoder Block을 사용한 AR모델)\n",
        "  - GPT2 모델에서 다시 10배 이상 데이터(570GB)와 100배 이상 모델 사이즈(175GB)를 키운 모델\n",
        "  - few-shor을 통한 in-context learning이 가능함을 발견"
      ],
      "metadata": {
        "id": "CtPcz2UAdQKm"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tNZAKzPAh5Dw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}